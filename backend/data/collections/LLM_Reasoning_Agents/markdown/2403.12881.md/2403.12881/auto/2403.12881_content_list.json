[
    {
        "type": "text",
        "text": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Zehui Chen 1 Kuikun Liu 2 Qiuchen Wang 1 Wenwei Zhang 2 Jiangning Liu 2 Dahua Lin 2 Kai Chen 2 Feng Zhao 1 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose AgentFLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by $3 . 5 \\%$ across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Language agent (Mialon et al., 2023; Sumers et al., 2023), which leverages the powerful abilities of LLMs to perceive environments, make decisions, and take actions, has emerged as an effective solution to complicated real-world problems (Song et al., 2023; Huang et al., 2023). Existing studies mainly focus on either prompt engineering (Wei et al., 2022; Yao et al., 2022b; Shinn et al., 2023) or framework scheduling of one/multiple close-sourced LLM (GPT4) (Wu et al., 2023; Talebirad & Nadiri, 2023; Hong et al., 2023) to accomplish agent tasks. Despite the impressive results and flexibility of these works, close-sourced LLMs suffer from steep financial cost and safety issues (Li et al., 2023; Yuan et al., 2023), impeding them from further deployment. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Recently, open-sourced LLMs emerged as effective alternatives and show promising results in various applications (Chiang et al., 2023; Touvron et al., 2023; Jiang et al., 2024). However, a lingering challenge persists: while these LLMs excel in tasks necessitating linguistic finesse, they fall short when deployed as agents, especially when compared to APIbased models (Zeng et al., 2023; Liu et al., 2023a). Efforts have been made to fine-tune LLMs for specific vertical fields (Qin et al., 2023; Gou et al., 2023), but this approach risks compromising their general capabilities. Thus, integrating effective agent abilities into general LLMs becomes a critical and pressing challenge in the field, aiming to bridge the gap between specialized and versatile language models. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this paper, we embark on a journey to mitigate this disparity between the prowess of open-sourced LLMs and the superior performance exhibited by API-based models in agent-centric tasks. We first unravel three critical observations that serve as the foundation for our exploration: (1) most agent training data is entangled with both format following and general reasoning, deviating from the original pretrain domain of the model, i.e., natural conversation, leading to inadequate agent learning, (2) by explicitly decomposing the training data along the basic capabilities, each loss exhibits different convergence curves, indicating varied learning speeds on the abilities requisite for agent tasks, and (3) the existing approaches that predominantly concentrate on specific agent abilities, overlooking the prevalence and significance of hallucinations in the model’s output. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Building upon these key insights, we present Agent-FLAN, which investigates the designing choice of effective Agent ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/0e846c19dfd8fc82b77ddfdea3ed49932acd45e93d08e1e8bda3ab9c3ec9fd4d.jpg",
        "image_caption": [
            "Figure 1. Comparison of recent agent tuning approaches on Held-In, Held-Out tasks. Performances are normalized with GPT-4 results for better visualization. \\* denotes our re-implementation for a fair comparison. "
        ],
        "image_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Finetuning for large LANguage models. Specifically, we tease apart format following and common reasoning in the agent training corpus, aligning the fine-tuning process to the pretrained domain of the language model. This elicits the pure agent abilities in LLMs without overfitting to specific format protocols. By further disentangling the agent tasks into distinct facets along the fundamental competencies of LLMs, Agent-FLAN affords training flexibilities depending on the varying learning rates of respective competencies. To comprehensively resolve the hallucination issues in agent tasks, we construct the Agent- $\\boldsymbol { \\cdot } \\boldsymbol { H }$ benchmark, which assesses the hallucination issues of LLMs from various aspects. Subsequently, we meticulously curate diverse ‘negative’ training samples to mitigate this problem effectively. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We employ Agent-FLAN on the open-sourced Llama2- series, which surpasses prior works by a substantial $3 . 5 \\%$ margin across a spectrum of agent evaluation benchmarks, including general agent tasks and tool utilization. Furthermore, we provide a deeper understanding of the dynamics involved in agent tuning: the scaling laws governing data and model dimensions, and the intricate relationships between general and agent-specific tasks. Our major contributions are as follows: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• We identify three critical observations that hinder opensourced LLMs from achieving competitive performance in the agent domain, offering valuable insights into the complex landscape of agent tuning. • Based on above findings, we introduce Agent-FLAN, an innovative approach aiming to integrate effective agent abilities into general LLMs: aligning agent tuning to chat format (§4.1), capabilities decomposition and data balancing (§4.2), and negative sample constructions for hallucination elimination (§4.3). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• Agent-FLAN outperforms prior works by a substantial $3 . 5 \\%$ margin on Llama2-series across a spectrum of agent evaluation benchmarks. Besides, we further study the dynamics of agent tuning, including scaling laws in terms of data and model scales, and intricate relationships between general and agent-specific tasks. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2. Related Work ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1. LLM as Agent ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The emergence of large language models (LLMs) represents a significant stride towards Artificial General Intelligence (AGI). With the advancement of LLMs, language agents built upon LLMs engage with the world to accomplish a diverse array of tasks, having become a focal point of research (Wang et al., 2023a; Xi et al., 2023). LLMs have been utilized in various studies for specific agent tasks, including web browsing(Deng et al., 2023; Zhou et al., 2023), online shopping (Yao et al., 2022a), database operations(Liu et al., 2023a), science experiments (Wang et al., 2022a), observation free reasoning (Xu et al., 2023), Wikipedia Q&A (Yang et al., 2018), daily computer tasks (Kim et al., 2023) and household exploration (Shridhar et al., 2020). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In addition to research focused on specific tasks, there are ongoing studies concerning AI agents based on LLMs. Some studies, such as ReAct(Yao et al., 2022b), emphasize actions during thinking, leading to significant improvements over various methods. While other works focus primarily on human and social property inside agents (Mao et al., 2023; Park et al., 2023; Zhang et al., 2023a), intelligence collaboration within multiple agents (Chen et al., 2023b; Liu et al., 2023b; Liang et al., 2023). Different from above, Agent-FLAN facilitates the integration of effective agent capabilities into general LLMs, enabling models to better understand and tackle complex problems in the real world. ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/5357c27e88b911663695809fd2ca252617ab7862816571d640700bf93de87f9e.jpg",
        "image_caption": [
            "Figure 2. Comparison of training loss on ReAct data (Toolbench (Qin et al., 2023)) and normal conversation (Flan2022 (Longpre et al., 2023)). "
        ],
        "image_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.2. Language Model Fine-Tuning ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Language Model Fine-Tuning is also a research hotspot, involving the adjustment of pre-trained models to adapt to specific tasks, aiming to align the output with expectations (Zhang et al., 2023b). Various research studies have been conducted on fine-tuning to optimize the model’s reasoning capabilities (Liu et al., 2021; Fu et al., 2023), proficiency in tools (Patil et al., 2023; Qin et al., 2023; Qiao et al., 2023), planning capability (Chen et al., 2023a), retrievalaugmented (Wang et al., 2023b), etc. Additionally, there are various studies conducted on fine-tuning methods (Hu et al., 2021; Ivison et al., 2022; Dettmers et al., 2023; Lv et al., 2023), data selection principles (Gunasekar et al., 2023) and fine-tuning datasets (Sanh et al., 2021; Wang et al., 2022b; Honovich et al., 2022; Longpre et al., 2023). ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3. Pilot Observations ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In this section, we delve into three pivotal observations on agent tuning that serve as the foundation of our subsequent investigation. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Observation 1. Most agent training data is entangled with both format following and general reasoning, causing a significant departure from the model’s original pretraining language domain, namely, natural conversation. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Recent agent tuning works (Zeng et al., 2023; Qin et al., 2023) endorse the adoption of specific formats, exemplified by ReAct (Thought-Action-Observation), for fine-tuning the language model. Moreover, it’s noteworthy that action arguments are frequently presented in JSON format. Encoding both format and reasoning knowledge into the training corpus shifts the tuning process from the original chat domain, presenting it as an out-of-domain task for language models. As shown in Figure 2, we compare the training curve of formatted data and normal data. It can be clearly seen that the loss associated with formatted data descends more rapidly to a low value, while keeping content loss still high (0.54 vs 0.04), indicating that the former leads to an inadequate learning process. This phenomenon can be probably attributed to the existence of a fixed structure (ReAct, JSON), where the model quickly gets overfitted to the format itself. Consequently, it fails to grasp the underlying reasoning abilities embedded within the training data, resulting in unsatisfied performance. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/2b0f4741902f879ff3a3c74619bf99a909982da5f619d24a6a7f517caebfffd3.jpg",
        "image_caption": [
            "Figure 3. Visualization of training loss by decomposing it into different capabilities of models: retrieval, instruct following, reasoning, and understanding. "
        ],
        "image_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Observation 2. By explicitly decomposing the training data along the basic capability aspects, each loss exhibits different convergence curves, indicating varied learning speeds on the capabilities requisite for agent tasks of LLMs. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Inspired by (Chen et al., 2023c), we explicitly disentangle the model’s capabilities into distinct components: instruction following, reasoning, retrieval, and understanding. In this context, instruction following corresponds to format generation, reasoning corresponds to the thought quality at each step, retrieval involves selecting the appropriate function name to execute the task, and the understanding encompasses the parameter inputs for the selected functions. By visualizing the loss based on respective aspects in Figure 3, we discern that LLM tends to exhibit varying learning speeds for the capabilities essential for proficient agents. To elaborate, retrieval and understanding emerge as relatively more manageable tasks compared to reasoning, with instruction following being the simplest in the learning process. This observation serves as a compelling motivation to further disentangle the training data along these model capabilities and subsequently balance these data based on the model’s varying learning rates. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Observation 3. Existing approaches predominantly concentrate on specialized agent abilities, overlooking the prevalence and significance of hallucination effects in the model’s output. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "AgentTuning (Zeng et al., 2023) introduces mixture train",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "(a) Format Hallucination ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/db610ef09ccca021542454f725dc63aca332bf9a7a735caaf139c63ddbbfd26e.jpg",
        "image_caption": [
            "Figure 4. Illustration of two typical hallucinations in general agent tasks for current open-sourced LLMs: (a) format hallucination and (b) action hallucination. "
        ],
        "image_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "ing, where both a general dataset and an agent dataset are simultaneously provided to the model during the tuning process. While this strategy indeed leads to steady performance improvements, we observe that it has limited impact on addressing hallucination issues, i.e., a crucial concern often overlooked in recent agent studies. This concern becomes particularly significant when deploying language models in real-world applications, as illustrated in Figure 4. Hallucination manifests in two primary aspects: (1) When the model is required to invoke a response, it strictly adheres to the training format, disregarding the user-generated query, and (2) The model is susceptible to being triggered with nonexistent functions when presented with induced questions. This underscores the necessity of directing more attention toward refining agent tuning mechanisms and the establishment of appropriate benchmarks to assess and mitigate agent hallucination effectively. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4. Agent-FLAN ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Recent works start to explore the effectiveness of finetuning language models on various agent tasks, with varied data quality, model sizes, and tuning approaches. Building on the pilot observations, we dive deeper into the data and method designing choice of effective agent tuning and discuss three key improvements to the language agent models. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Experimental Setup We finetune the language model Llama2-series (Touvron et al., 2023), and use 7B size for ablations for efficiency unless otherwise stated. We construct the dataset in accordance with the data and settings established by AgentTuning (Zeng et al., 2023). Specifically, we pick a suite of training data sources as held-in tasks: ALFWorld (Shridhar et al., 2020), WebShop (Yao et al., 2022a), Mind2Web (Deng et al., 2023), Knowledge Graph (Liu et al., 2023a), Operating System (Liu et al., 2023a), Database (Liu et al., 2023a), and ToolBench (Qin et al., 2023), covering both general agents and tool utilization domains. Our held-out evaluation encompasses a list of complex interactive tasks, including complex QA (HotpotQA (Yang et al., 2018)), web browsing (WebArena (Zhou et al., 2023)), science experiments (SciWorld (Wang et al., 2022a)), and tool utilization (T-Eval (Chen et al., 2023c)). Details adopted in Agent-FLAN and hyper-parameters during training can be found in Appendix A. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4.1. Aligning Agent Tuning to Pretrain Domain ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "LLMs are first pretrained on natural language corpus, which comprises a massive number of human conversations. However, agent data are often presented in specific formats (ReAct, JSON), leading to out-of-distribution learning during the tuning process. Such a misalignment further results in an inadequate learning process. Besides, LLMs are more likely to get overfitted to these specific formats after finetuning, deteriorating their instruction-following abilities. To mitigate this problem, we propose to transform the formatted data into natural conversations. Concretely, we first ",
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/10b33de77de7811cd8c4308f7712147a749a0240285b7ad99e687702714cdf36.jpg",
        "table_caption": [
            "Table 1. Main results of Agent-FLAN. Agent-FLAN significantly outperforms previous agent-tuning approaches by a large margin on both held-in and held-out tasks. \\* denotes our re-implementation with the same amount of training data for a fair comparison. Since FireAct does not train on AgentInstruct dataset, we omit its performance on the HELD-IN set. Bold: the best in API-based and open-sourced models. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">MODEL</td><td rowspan=\"2\">HELD-IN</td><td colspan=\"6\"></td></tr><tr><td>HotpotQA</td><td>SciWorld</td><td></td><td></td><td>Agent-H</td><td>Overall</td></tr><tr><td>GPT-3.5 (OpenAI, 2022)</td><td>1.59</td><td>37.4</td><td>21.2</td><td>4.56</td><td>84.0</td><td>92.1</td><td>47.8</td></tr><tr><td>GPT-4 (OpenAI, 2023)</td><td>2.75</td><td>52.1</td><td>36.4</td><td>6.28</td><td>86.4</td><td>94.2</td><td>55.1</td></tr><tr><td>Llama2-7B (Touvron et al., 2023)</td><td>0.19</td><td>22.6</td><td>5.9</td><td>1.2</td><td>27.4</td><td>78.7</td><td>27.2</td></tr><tr><td>FireAct-7B (Chen et al., 2023a)</td><td>-</td><td>26.2</td><td>6.8</td><td>0.25</td><td>9.3</td><td>40.4</td><td>16.6</td></tr><tr><td>AgentLM-7B (Zeng et al., 2023)</td><td>1.96</td><td>22.3</td><td>13.7</td><td>0.74</td><td>41.4</td><td>80.6</td><td>31.7</td></tr><tr><td>AgentTuning* (Zeng et al., 2023)</td><td>1.89</td><td>25.4</td><td>16.8</td><td>2.71</td><td>61.8</td><td>84.5</td><td>38.2</td></tr><tr><td>Agent-FLAN (Ours)</td><td>2.01</td><td>28.5</td><td>20.0</td><td>4.68</td><td>66.0</td><td>89.1</td><td>41.7</td></tr></table>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Original Agent Corpus ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/90b5cef48df35b1e023655a201a83404098720a260f9348cca36eb08f8513643.jpg",
        "image_caption": [
            "Figure 5. By aligning the original agent corpus to the natural conversation, we are able to explicitly decompose agent tasks into different capabilities, leading to more fine-grained data balancing. "
        ],
        "image_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "replace the classical ‘THOUGHT-ACTION-ACTIONINPUT’ templates with multi-turn dialogues. After that, we further decompose JSON arguments by inserting several elicit statements. An example has been shown in Figure 5. Since loss will be only applied to the ‘assistant’ clause, the introduction of formatted elicit statements has little overfitting issue on models. By explicitly aligning the agent corpus into chat domain, we are able to fully boost the learning on pure agent ability, without focusing on strict format protocols. In order to keep the ability to output various requested formats, we additionally construct instruction following pairs which request the model to respond with ReAct and JSON formats. Our latter experiments also show that only a small portion of instruct following data is enough to achieve satisfying results. From Table 2, we can observe steady improvements by aligning training corpus into chat domain, i.e., $3 . 1 \\%$ improvements on T-Eval, and $2 . 5 \\%$ on HotpotQA. This further validates the correctness and effectiveness of the alignment of training corpus to the chat format. ",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/c565839b0f0d881ce59bfd2e6d0434f95f2d9c6af086870fef4ada87ef2f2fa7.jpg",
        "table_caption": [
            "Table 2. Subsets of decomposed capabilities are left out with half from an equally weighted mixture to measure their importance. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Methods</td><td>Tokens (M)</td><td>T-Eval</td><td>HotpotQA</td></tr><tr><td>ReAct (Baseline)</td><td>19.2</td><td>61.8</td><td>25.4</td></tr><tr><td>Align-Chat (All))</td><td>37.3</td><td>64.9</td><td>27.9</td></tr><tr><td>All - Reasoning</td><td>32.4</td><td>63.8</td><td>27.4</td></tr><tr><td>All - Retrieval</td><td>36.2</td><td>65.3</td><td>29.0</td></tr><tr><td>All - Understand</td><td>35.4</td><td>64.6</td><td>28.1</td></tr><tr><td>All - Inst.</td><td>28.4</td><td>65.9</td><td>27.5</td></tr><tr><td>AlI (Weighted)</td><td>18.1</td><td>66.3</td><td>28.5</td></tr></table>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4.2. Capabilities Decomposition and Data Balancing ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Prior works have shown that a proper mixture of training data sources leads to better performance (Longpre et al., 2023). In this work, instead of simply exploring the balanced composition of each dataset, we investigate the mixture of training corpus from the perspective of capabilities. Inspired by (Chen et al., 2023c), we explicitly decompose the agent data along the capabilities required by each task, including reasoning, retrieval, understanding, and instruction following. As demonstrated in Section 3, LLM exhibits varied learning speeds on each capability, indicating that properly composing these data sources also matters in optimizing final results. To validate this assumption, we conduct experiments with vanilla decomposed aligned agent corpus, where each ability data is identical to the original data, and then ablates each sub-mixture with half of the data on T-Eval and HotpotQA. ",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/05a496cd3f191a57f990d22d05bc77dc823192eab8ba1e4ebb4bf10bee56d31d.jpg",
        "image_caption": [
            "Figure 6. Demonstration of 4 different circumstances commonly existed in agent tasks, by splitting them through user query and system prompt. "
        ],
        "image_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "As shown in Table 2, reasoning and understanding are among the most beneficial mixtures, then comes retrieval and instruction following: when reducing the portion of reasoning and understanding data to $50 \\%$ , the final performance declines 1.1/0.3 points, respectively. By reducing the amount of retrieval and instruction following, the performance incurs little influence, and even improves. These findings are also consistent with the observations in Figure 3, where the loss of retrieval and instruction following drops much quicker than reasoning and understanding. Such a phenomenon also inspires us to narrow the mixture weights search space and greatly reduce the training tokens based on the loss curves on each capability. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4.3. Negative Sample Learning for Hallucination Elimination ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Hallucination is a crucial issue of current LLMs, which refers to “generating unfaithful or nonsensical text” (Ji et al., 2023). In agent tasks, we also observe such phenomenons as illustrated in Figure 4. We summarize agent hallucination into two main categories: format hallucination and action hallucination. The former plays an important role when deploying LLMs into specific agent systems, and the latter matters when acting as a general chat assistant. Therefore, how to effectively eliminate the agent hallucination issues is an essential path in developing agent LLMs. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "However, most prior work mainly focuses on the general agent’s abilities, while omitting the hallucination issues. To comprehensively gauge the LLMs on agent hallucination, we first establish Agent- $H$ benchmark, which accesses such issues from two aspects: (1) format-level: requests with various response formats, and check if the model follows the instructions, and (2) action-level: we curate the questions from 4 different perspectives as illustrated in Figure 6, covering most agent circumstances for LLMs. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Specifically, we select glaive-function-calling-v2 (GlaiveAI, ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2023) as our base dataset. By explicitly checking if the response contains tool invocation, we curate 1845 samples for out-of-domain validation. Since we focus on the hallucination issues when acting as an agent, the evaluation protocol only gauges if the output of the model is a raw response or specified function calling. Specifically, we define two specific format checkings: (1) ReAct-format hallucination (e.g., ‘Thought:’, ‘Action:’), and (2) general-format hallucination (e.g., ‘I will use’, ‘I need to call’). If the response contains the above keywords, it will be viewed as one hallucination failure when the ground truth is a raw response type. Based on these, we further define two numerical metrics: $\\mathrm { H } _ { \\mathrm { R e A c t } }$ and $\\mathrm { H } _ { \\mathrm { G e n e r a l } }$ as the number of respective format hallucinations / number of raw responses in the ground truth. The final overall score $\\mathrm { H } _ { \\mathrm { S c o r e } }$ is a reverse average of the above two metrics: ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/064ce56fd3edfae2b262c39b77b39cc67bd31533458069183526b818f11b6491.jpg",
        "text": "$$\n\\mathrm { H _ { S c o r e } } = 0 . 5 * \\left( \\left( 1 - \\mathrm { H _ { R e A c t } } \\right) + \\left( 1 - \\mathrm { H _ { R e A c t } } \\right) \\right)\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table 3 reports the experimental results on Agent- $H$ , as well as the scores on T-Eval, which provides a comprehensive demonstration of both agent abilities and hallucination issues. Ideally, a general language model should obtain high scores on both benchmarks. From the table, we can see that Llama2-7B obtains low scores on both Agent- $H$ and T-Eval. This is possibly due to the lack of agent data in its pretraining corpus, which further proves the necessity of agent tuning. We also follow the implementation of AgentTuning (Zeng et al., 2023) to finetune the model from Llama2-7B. Despite the huge improvements in T-Eval score, the hallucination issue is quite severe gauged by Agent-H, pinpointing the inner defects of current agent tuning approaches. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "To address this problem, we first examine the current agent corpus according to Figure 6. It is easy to find that in most cases, the training data only covers normal conversation without tools (a) and agent tasks with provided tools (d) while omitting the rest (b,c). Since the model has never seen these negative samples during training, it can hardly generalize to these requests, leading to unexpected responses. To this end, we introduce negative sample learning, by meticulously curating diverse negative training samples covering various conditions mentioned above. Specifically, we insert two different types of negative samples: (1) no tools provided, user query requests for tools (2) tool provided, user query requests for normal conversations. Through explicit supervision, we teach the model not only how but when to act as an agent. In Table 3, the negative sampling learning strategy significantly mitigates the hallucination issues, meanwhile maintaining the high performance on T-Eval. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5. Analysis ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In this section, we conduct in-depth experiments to analyze ",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/ddb59d9c8435a213b528d51b4fb034262447356d5a0f7be52b07835d2247abdb.jpg",
        "table_caption": [
            "Table 3. Experimental results on Agent-H with Llama2-7B. $\\mathrm { H } _ { \\mathrm { s c o r e } }$ is the overall score of Agent-H benchmark. “NS” denotes negative training samples. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">T-Eval</td><td colspan=\"3\">Agenta I Hsco T</td></tr><tr><td>HReAct ↓</td><td></td><td></td></tr><tr><td>Llama2-7B</td><td>27.4</td><td>21.7</td><td>21.0</td><td>78.7</td></tr><tr><td>AgentTuning</td><td>61.8</td><td>18.1</td><td>14.0</td><td>83.9</td></tr><tr><td>Agent-FLAN</td><td>66.0</td><td>9.9</td><td>11.9</td><td>89.1</td></tr><tr><td>w/o NS</td><td>66.3</td><td>15.6</td><td>13.5</td><td>84.5</td></tr></table>",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.1. Scaling Law for Agent Tuning ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Scaling law for language models is quite important and provides valuable insights in further development (Longpre et al., 2023; Chung et al., 2022). In this section, we explore such phenomenons on agent tuning from both the data and model scales on the general HotpotQA task. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.1.1. DATA SCALING LAW FOR AGENT TUNING ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Recent work (Chung et al., 2022) has demonstrated language models can benefit from vast and diverse training corpus in general abilities. We investigate how the amount of training data influences the agent’s abilities. By evenly dividing Agent-FLAN data into $2 5 \\%$ , $50 \\%$ , $7 5 \\%$ , and $100 \\%$ we report results in Figure 7. It can be seen that with only $2 5 \\%$ training samples, the agent ability gains the most. This further verifies that the vanilla Llama-2 model exhibits weak agent abilities and requires specific training, and only a small portion of agent corpus can elicit most abilities required by agent tasks. When further increasing the data amount $( 5 0 \\% , 7 5 \\% )$ , the improvements persist, however with slower speeds, which indicates that simply enlarging the scale of the training corpus of agents does not contribute much to the model capabilities. Therefore, enriching the diversity or improving the quality of the training corpus may be the necessary path to better language agents. ",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/e8c9f61d782edaa17dbfae39dc23c47fdf416be7e132f7b39fa8d877b14d8801.jpg",
        "image_caption": [
            "Figure 7. Performance scaling laws for the amount of training data, ranging from $0 \\%$ to $100 \\%$ . "
        ],
        "image_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.1.2. MODEL SCALING LAW FOR AGENT TUNING ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "As previous work (Longpre et al., 2023) proves that zero/few-shot ability of language models is substantially improved with a larger model scale, we next explore if this rule is also applied in the agent domain. By evaluating our approach on models of size 7B, 13B, and 70B on Llama2, the final results are shown in Figure 8. We can observe that as the model scale increases, the performance continuously improves without any saturation, proving that larger parameters do guarantee better performances. When taking a close look at enhancement across different model scales, it can be found that specific agent tuning brings steady improvements compared to vanilla ReAct tuning as the model scales up. We infer the reason that larger models have already possessed basic abilities required by agent tasks, such as reasoning and retrieval. Therefore, it is more important to elicit agent capability with a certain amount of agent tuning corpus in an appropriate way. ",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/f1b79b55fe373da53103bb322856835867040cfd624186fa9fe43748f2e3b867.jpg",
        "image_caption": [
            "Figure 8. Performance scaling laws for the parameters of training models, from Llama2-7B to Llama2-70B. "
        ],
        "image_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.2. General Ability v.s Agent Ability ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "As we have seen that specific tuning improves the ability of a model to act as agent, it is still unclear what is the relationship between general ability and agent ability. AgentTuning has verified the necessity of general data on agent training, but does agent tuning further promote the general capability of the model? We evaluate our model on three general capabilities that are widely adopted in the field: MMLU (linguistic knowledge), GSM8K (mathematical ability), and HumanEval (code capability). The results are shown in Table 4. From the table, we can see that introducing agent training corpus not only enhances the ability on agent tasks but also brings extra benefits to general capabilities. We speculate the reason that agent corpus contains basic capabilities such as reasoning, and instruction following, which are also amenable to other domains. This further indicates that properly integrating agent data into the current LLM training corpus can lead to better performance. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "(a) ToolBench ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "INsTRucrion: You are a helpful assistant.   \nAnswer the following questions as best you can. You have access to the following tools:   \n{'name': 'reverse geocoding', 'parameters': {Format Instruction...}   \n{other Tool Information...}   \nQuEry: I'm planning a road trip from Madrid to Barcelona. Can you provide me with the step-bystep directions, distance?... ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "AgentTuning ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "...(Previous Interactions) Wrong Parameters ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "RESPONSE: ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Action: reverse_geocoding Args: {\"coordinate\": \"-3.703823, 40.416631\"} ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Agent-FLAN ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "...(Previous Interactions) Correct   \nRESPONSE:   \nAction:reverse_geocoding   \nArgs:{\"1on\":−3.703823\", \"1at\": \"40.416631\"} ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "(b) Agent-H ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "InsTRucrion: You are a helpful assistant.   \nAnswer the following questions as best you can. You have access to the following tools:   \n{Format Instruction...}   \n{other Tool Information...}   \nQuERy: Can you book a movie ticket for me? ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "AgentTuning ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "RESPONSE: meaningless tool usage Action: get_movie_details Args: {\"title\":The Shawshank Redemption\"} ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Agent-FLAN ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "RESPONSE: ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Correct ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "I'm sorry, but as an AI language model, I don't have the ability to book movie tickets. However, you can easily book a movie ticket online or through a mobile app. Just search for the movie you want to watch and the theater near you, and follow the instructions to book your ticket. ",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "",
        "image_caption": [
            "Figure 9. Comparison studies on Toolbench and Agent-H datasets between AgentTuning and Agent-FLAN with Llama2-7B. (a) ToolBench: Thanks to the capability decomposition and more focus tuning on ‘understand’, Agent-FLAN is able to catch up with the specific API information given long tool information content, whereas AgentTuning failed with hallucination. (b) Agent-H: the AgentTuning model presents a meaningless tool usage while Agent-FLAN directly gives the preferred response. "
        ],
        "image_footnote": [],
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/b52bb06709f947993d817de062250dd8d272a03258defcea2605a1df31cc0b54.jpg",
        "table_caption": [
            "Table 4. Effectiveness of AgentFLAN corpus on the general capabilities with Llama2-series. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">DATA</td><td colspan=\"3\">GSMK-  HUMANEVAL</td></tr><tr><td>MMLU</td><td></td><td></td></tr><tr><td>general data</td><td>50.0</td><td>21.9</td><td>15.1</td></tr><tr><td>Agent-FLAN</td><td>49.7 -0.3</td><td>22.1 +0.2</td><td>15.5 +0.4</td></tr><tr><td rowspan=\"2\">DATA</td><td></td><td>Llama2-13B</td><td></td></tr><tr><td>MMLU</td><td>GSM8K</td><td>HUMANEVAL</td></tr><tr><td>general data Agent-FLAN</td><td>54.7 55.8 +1.1</td><td>34.8</td><td>15.2</td></tr><tr><td></td><td></td><td>35.2 +0.4</td><td>15.8 +0.6</td></tr><tr><td>DATA</td><td>MMLU</td><td>Llama2-70B GSM8K</td><td>HUMANEVAL</td></tr><tr><td>general data</td><td>68.0</td><td>64.5</td><td>32.1</td></tr><tr><td>Agent-FLAN</td><td>68.5 +0.5</td><td>64.6 +0.1</td><td>32.9 +0.8</td></tr></table>",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "6. Conclusion ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this work, we investigate the design choice of data and methods in agent tuning for LLMs. By first carrying out three critical observations, we pinpoint the current bottleneck in bridging the gap between the open-sourced LLMs and API-based models. Based on this, we present AgentFLAN to effectively fine-tune the language models for agent tasks. Through careful decomposition and redesign of the existing training corpus, Agent-FLAN empowers Llama2- 7B to outperform previous works significantly on a wide ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "spectrum of agent tasks. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "7. Limitations ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this paper, we focus on constructing the agent training corpus. Despite our best efforts, this paper may still have some remaining limitations. 1) The training and validation dataset only encompass part of agent tasks. There are many other interactive scenarios beyond this. We will conduct further research on applying Agent-FLAN to a wider range of benchmarks in the future. 2) In order to keep the quality of training data, we only choose roughly 20,000 valid samples from ToolBench, which is $10 \\%$ of the whole dataset. It would be better to fully utilize them to further enhance the performance of the model. We leave it for future work. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "8. Ethical Considerations ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "We used publicly available reference documents/APIs for our experiments, effectively circumventing any possible harm toward individuals or groups. The generated data by LLMs were carefully selected and processed by humans to secure privacy and confidentiality. No personal identification information was involved, and all data were made anonymous before any analysis was conducted. Besides, We use ChatGPT and Grammarly to polish the writing. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "References   \nChen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. FireAct: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023a.   \nChen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. AgentVerse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023b.   \nChen, Z., Du, W., Zhang, W., Liu, K., Liu, J., Zheng, M., Zhuo, J., Zhang, S., Lin, D., Chen, K., et al. T-eval: Evaluating the tool utilization capability step by step. arXiv preprint arXiv:2312.14033, 2023c.   \nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with $9 0 \\% \\ast$ chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/.   \nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.   \nDeng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.   \nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.   \nFu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023.   \nGlaiveAI. glaive-function-calling-v2, 2023. URL https://huggingface.co/datasets/ glaiveai/glaive-function-calling-v2.   \nGou, Z., Shao, Z., Gong, Y., Yang, Y., Huang, M., Duan, N., Chen, W., et al. ToRA: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.   \nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.   \nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. MetaGPT: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.   \nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.   \nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, pp. 1769–1782. PMLR, 2023.   \nIvison, H., Bhagia, A., Wang, Y., Hajishirzi, H., and Peters, M. Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation. arXiv preprint arXiv:2212.10315, 2022.   \nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38, 2023.   \nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \nKim, G., Baldi, P., and McAleer, S. Language models can solve computer tasks, 2023.   \nLi, H., Guo, D., Fan, W., Xu, M., and Song, Y. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023.   \nLiang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S. Encouraging divergent thinking in large language models through multi-agent debate, 2023.   \nLiu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., and Hajishirzi, H. Generated knowledge prompting for commonsense reasoning. arXiv preprint arXiv:2110.08387, 2021.   \nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. AgentBench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023a.   \nLiu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy, R., Feng, Y., Chen, Z., Niebles, J. C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong, C., and Savarese, S. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents, 2023b.   \nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.   \nLv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782, 2023.   \nMao, S., Zhang, N., Wang, X., Wang, M., Yao, Y., Jiang, Y., Xie, P., Huang, F., and Chen, H. Editing personality for llms. arXiv preprint arXiv:2310.02168, 2023.   \nMialon, G., Dess\\`ı, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi- \\` Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.   \nOpenAI. Openai: Introducing chatgpt, 2022. URL https: //openai.com/blog/chatgpt.   \nOpenAI. Gpt-4 technical report, 2023.   \nPark, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 1–22, 2023.   \nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.   \nQiao, S., Gui, H., Chen, H., and Zhang, N. Making language models better tool learners with execution feedback. arXiv preprint arXiv:2305.13068, 2023.   \nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. ToolLLM: Facilitating large language models to master $1 6 0 0 0 +$ real-world apis. arXiv preprint arXiv:2307.16789, 2023.   \nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.   \nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nShridhar, M., Yuan, X., Cotˆ e, M.-A., Bisk, Y., Trischler,´ A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.   \nSong, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2998–3009, 2023.   \nSumers, T. R., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.   \nTalebirad, Y. and Nadiri, A. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.   \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023a.   \nWang, R., Jansen, P., Cotˆ e, M.-A., and Ammanabrolu, P. ´ Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11279– 11298, 2022a.   \nWang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., Zheng, R., Ye, J., Zhang, Q., Gui, T., et al. Instructuie: Multi-task instruction tuning for unified information extraction. arXiv preprint arXiv:2304.08085, 2023b.   \nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b.   \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824–24837, 2022.   \nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.   \nXi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Xu, B., Peng, Z., Lei, B., Mukherjee, S., Liu, Y., and Xu, D. Rewoo: Decoupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323, 2023. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, 2018.   \nYao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744–20757, 2022a.   \nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022b.   \nYuan, Y., Jiao, W., Wang, W., Huang, J.-t., He, P., Shi, S., and Tu, Z. GPT-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463, 2023.   \nZeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J. AgentTuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.   \nZhang, J., Xu, X., and Deng, S. Exploring collaboration mechanisms for llm agents: A social psychology view, 2023a.   \nZhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., and Wang, G. Instruction tuning for large language models: A survey, 2023b.   \nZhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "A. Experimental Details ",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "For Agent-FLAN experiments, we follow the practice in AgentTuning, where ShareGPT and agent corpus are mixed for training with a mix ratio of 1:1. Besides, we empirically find that a small amount of ReAct format data improves the performance. Therefore, we incorporate $10 \\%$ data with ReAct format and the rest follows the format in Agent-FLAN. The models are fine-tuned on the training corpus with 1 epoch. We list the hyper-parameters of models with different scales in Table 5. The balancing weight for reasoning, retrieval, and understanding is set to 1:0.25:0.75. We additionally curate 2000 samples of instruction following data for training. ",
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/890e99e1f7858e3dab0ea71a282dac7d7096eb7f276bafbf19c3a77ae645c5f1.jpg",
        "table_caption": [
            "Table 5. Experimental settings and hyper-parameters for Llama2- series. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Hyper-Params</td><td>Llama2-7B</td><td>Llama2-13B</td><td>Llama2-70B</td></tr><tr><td>GPUs</td><td>16</td><td>32</td><td>128</td></tr><tr><td>Learning Rate</td><td>2e-5</td><td>2e-5</td><td>2e-5</td></tr><tr><td>WarmUp LR</td><td>4e-6</td><td>4e-6</td><td>4e-6</td></tr><tr><td>Batch Size</td><td>32</td><td>32</td><td>128</td></tr><tr><td>LR Schedule</td><td>Cosine</td><td>Cosine</td><td>Cosine</td></tr></table>",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "B. Dataset Details ",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In this section, we provide details on the detailed construction of our constructed Agent-FLAN dataset. In order to keep the generality of our approach, we adopt open-sourced datasets AgentInstruct (Zeng et al., 2023) (which comprises 6 different agent tasks), ToolBench (Qin et al., 2023) as our basic datasets. We filter the data that does not follow the format of ‘Thought-Action-ActionInput’, and remove the few-shot examples in the AgentInstruct. As for ToolBench dataset, we only keep samples that have FINALANSWER and remove those that directly begin with ‘This is not the first time you try this task’ (due to the DFSDT introduced in (Qin et al., 2023)). By default, Agent-FLAN remixes $10 \\%$ ReAct format and $90 \\%$ conversation format data for better performance. The overview of our dataset is shown in Table 6. ",
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/ef829b3fb294478abffbcec60e79f1c9580a1ac2f148aa74348400daa006cab2.jpg",
        "table_caption": [
            "Table 6. Dataset statistics used in Agent-FLAN. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Datasets</td><td>Sample Num</td></tr><tr><td>OS</td><td>195</td></tr><tr><td>Database</td><td>538</td></tr><tr><td>Knowledge Graph</td><td>300</td></tr><tr><td>Webshop</td><td>351</td></tr><tr><td>AlfWorld</td><td>336</td></tr><tr><td>Mind2Web</td><td>116</td></tr><tr><td>ToolBench</td><td>22,867</td></tr><tr><td>Total</td><td>24,703</td></tr></table>",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "C. Negative Samples Construction Details ",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In order to construct negative samples for hallucination elimination, we first extract a small portion (761 samples) of user queries from ToolBench dataset. Then we prompt the gpt-3.5-turbo to directly reply to these queries without providing any tool information. To construct normal conversation with the tool information provided, we randomly append half of the queries with irrelevant tools, which generate the case (b) in Figure 6. Illustrations of our constructed negative samples are shown in Figure 10, 11. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Negative Sample I ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "System:   \nYou are a helpful assistant.   \nUser Query:   \nI’m a crypto blogger and I’m writing an article about The Sandbox - SAND. Can you provide me with the price data for the past 1 week, 1 year, and 3 months? It would be great to include the market caps and total volumes as well. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Assistant: ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "As an AI language model, I don’t have real-time data. However, you can easily find the price data for The Sandbox (SAND) on various cryptocurrency tracking websites like CoinMarketCap or CoinGecko. These platforms provide historical price data, market caps, and total volumes for different time periods. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Negative Sample II ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "System: ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "You are a helpful assistant. You can use the following tools: ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "{ ’name’: ’get_recipe’, ’description’: ’Get a recipe based on given ingredients’, ’parameters’: {’type’: ’object’, ’properties’: {’ingredients’: {’type’: ’array’, ’items’: {’type’: ’string’}, ’description’: ’The list of ingredients available’}}, ’required’: [’ingredients’]}   \n},   \n{ ’name’: ’convert_weight’, ’description’: ’Convert weight from one unit to another’, ’parameters’: {’type’: ’object’, ’properties’: {’weight’: {’type’: ’number’, ’description’: ’The weight value’}, ’from_unit’: {’type’: ’string’, ’description’: ’The unit to convert from’}, ’to_unit’: {’type’: ’string’, ’description’: ’The unit to convert to’}}, ’required’: [’weight’, ’from_unit’, ’to_unit’]}   \n} ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "User Query: ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "I am planning a big event for my company and I need to minimize costs. Can you assist me in finding the latest coupons, trending coupons, and popular coupons for event venues, catering services, and transportation? Please provide the coupon codes, discount percentages, and expiration dates for each coupon. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Assistant: ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "I’m sorry, but as an AI language model, I don’t have real-time access to current coupons or their details. I recommend checking popular coupon websites or contacting specific event venues, catering services, and transportation providers directly for any available discounts or promotions. ",
        "page_idx": 12
    }
]