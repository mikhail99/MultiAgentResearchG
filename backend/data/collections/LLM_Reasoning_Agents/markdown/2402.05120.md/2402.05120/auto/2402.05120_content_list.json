[
    {
        "type": "text",
        "text": "More Agents Is All You Need ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Junyou Li∗ Tencent ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "junyouli@tencent.com ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Qin Zhang∗ Tencent ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "adrienzhang@tencent.com ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yangbin Yu Tencent ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "yangbinyu@tencent.com ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Qiang Fu Tencent ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "leonfu@tencent.com ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Deheng Ye† Tencent ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "dericye@tencent.com ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Reviewed on OpenReview: https: // openreview. net/ forum? id= bgzUSZ8aeg ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method, termed as Agent Forest, is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Although large language models (LLMs) demonstrate remarkable capabilities in variety of applications (Zhao et al., 2023), such as language generation, understanding, and reasoning, they struggle to provide accurate answers when faced with complicated tasks. To improve the performance of LLMs, some of recent studies focus on ensemble methods (Wang et al., 2023b; Wan et al., 2024) and multiple LLM-Agents collaboration frameworks (Du et al., 2023; Wu et al., 2023). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In these works, multiple LLM agents are used to improve the performance of LLMs. For instance, LLM-Debate Du et al. (2023) employs multiple LLM agents in a debate form. The reasoning performance is improved by creating a framework that allows more than one agent to “debate” the final answer of arithmetic tasks. They show performance improvements compared to using one single agent. Similarly, CoT-SC (Wang et al., 2023b) generates multiple thought chains and picks the most self-consistent one as the final answer. The reasoning performance is improved by involving more thought chains compared to chain-of-thought (CoT) (Wei et al., 2022) which employs a single thought chain. Incidentally, from the data analysis of these works, we can notice the effects of putting multiple agents together, to some extent, can lead to a performance improvement in certain problems. For example, in Table 10 of Section 3.3 of LLM-Debate Du et al. (2023), the authors have reported a preliminary curve: the accuracy of a math problem increases with the number of debating agents (although the number was simply increased from 1 to 7). Also, in Wang et al. (2023b), involving more chain-of-thought pipelines (termed as a “sample-and-marginalize” decoding procedure), can lead to a performance gain. We realize that the LLM performance may likely be improved by a brute-force scaling up of the number of agents instantiated. However, since the scaling property of “raw” agents is not the focus of these works, the scenarios/tasks and experiments considered are limited. So far, there lacks a dedicated in-depth study on such a phenomenon. Hence, a natural question arises: Does this phenomenon generally exist? ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/b2068708e93190eeadecfd27248956b641636c6fd2633dae7aecaeb4e51569e9.jpg",
        "image_caption": [
            "Figure 1: The accuracy increases with ensemble size across Llama2-13B, Llama2-70B and GPT-3.5-Turbo in GSM8K. When the ensemble size scales up to 15, Llama2-13B achieves comparable accuracy with Llama2-70B. Similarly, When the ensemble size scales up to 15 and 20, Llama2-70B and GPT-3.5-Turbo achieve comparable accuracy with their more powerful counterparts. The error bars represent the standard error. "
        ],
        "image_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "To answer the research question above, we conduct the first comprehensive study on the scaling property of LLM agents. To dig out the potential of multiple agents, we propose to use a simple(st) sampling-and-voting method, which involves two phases. First, the query of the task, i.e., the input to an LLM, is iteratively fed into a single LLM, or a multiple LLM-Agents collaboration framework, to generate multiple outputs. Subsequently, majority voting is used to determine the final result. The procedure is inspired by that of the CoT-SC, but it does not rely on designing complex CoT paths. In fact, it can be used as a plug-in to further enhance CoT-based methods, as will be shown in our evaluations. Our method is termed as Agent Forest, a tribute to the classic Random Forest (Breiman, 2001). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The experiments are conducted by using various LLMs of different sizes on diverse datasets covering reasoning and generation. The result indicates that LLM performance can generally be improved by increasing the ensemble size, i.e., the number of agents, across a wide range of tasks. Surprisingly, a brute-force ensemble of smaller LLMs can achieve comparable or superior performance to larger LLMs, with a nutshell shown in Figure 1, which will be further expanded in later sections. Moreover, by combining our method with other existing methods, we find the performance can be further improved. By comparing with the performance of complicated methods, the result shows that employing our method solely can achieve comparable performance in most cases. This implies that comparable performance can be achieved without the need for additional handcraft prompt design or complex collaboration frameworks. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Additionally, the experimental results indicate that there are greater performance improvements when addressing difficult tasks and when using weaker models. To understand the reasons behind these performance improvements, we analyze the influence of problem difficulty on the effectiveness of our method. We classify difficulty into three dimensions: the inherent difficulty, the length of reasoning steps, and the prior probability of the correct answer. Through a series of experiments, we adjust these dimensions and observe their effects independently. We observe and summarize a few properties, based on which, we further develop optimization strategies that can intrigue the power of “More Agents”. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Our contributions are summarized as follows: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• We present the first systematic study on the scaling property of raw agents instantiated by LLMs. We find that the performance scales with the increase of agents, using the simple(st) way of sampling and voting.   \n• We explore the compatibility of our method with existing complicated methods that stimulate the potential of LLMs, revealing that our method can enhance these methods to achieve further performance improvements.   \n• We analyze the effectiveness of our method in tackling problems at varying difficulties and then distill the properties behind, based upon which, we propose further optimization methods that can facilitate the occurrence of our finding. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 Related Work ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Related works can be categorized into three parts: 1) LLM self-ensemble Wang et al. (2023b), which attempts to harness multiple outputs from homogeneous LLMs to assemble the final answer; 2) heterogeneous LLM ensemble, which focuses on combining heterogeneous LLMs through supervised learning to improve performance across various downstream applications; and 3) multiple LLM agents collaboration, which improves performance through interactions among LLM agents. We discuss these works below. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "LLM Self-Ensemble. CoT-SC Wang et al. (2023b) harnesses diverse chain-of-thought Wei et al. (2022) prompts to elicit a variety of reasoning processes from a single LLM and select the final answer through majority voting. Fu et al. (2023); Li et al. (2023b); Cobbe et al. (2021b); Thoppilan et al. (2022); Lin et al. (2023) can be considered as the extensions of CoT-SC. These methods mainly focus on reasoning tasks and exclusively investigate the compatibility with CoT. In contrast, our method not only validates effectiveness in reasoning tasks but also in generation tasks. Moreover, our method is compatible with a broader range of methods, such as prompt engineering (including CoT) and multiple LLM agents collaboration. Very recently, Lu et al. (2024) proposes a method named Blended that utilizes multiple LLMs for chat scenarios. In contrast, Blended focuses on utilizing the power of multiple LLMs, whereas our focus is on the scaling trend of adding more LLMs. Also, Blended is only for limited chat scenarios evaluated via human annotations. Furthermore, we explore orthogonality with other methods. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Heterogeneous LLM Ensemble. Wan et al. (2024) conducts a supervised LLM fusion framework to distill multiple heterogeneous LLMs into a single model and surpasses each of these LLMs. Jiang et al. (2023) introduces a supervised ensembling framework based on multiple heterogeneous LLMs. Chen et al. (2023b) proposes a sequential inference method for LLMs that halts when the output quality is deemed adequate. Wang et al. (2023a) addresses the fusion-of-experts problem by integrating outputs from models with distinct knowledge domains through supervised learning. Shnitzer et al. (2023) and Lu et al. (2023) select the most suitable LLM for new tasks by training a reward-guided router. These approaches primarily employ supervised learning, necessitating task-specific annotated data, and exhibit limited generalizability. In contrast, our method is unsupervised, without the need for additional training data. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Multiple LLM Agents Collaboration. Du et al. (2023); Liang et al. (2023); Xiong et al. (2023) explore various multiple LLM agents interaction architectures, with employing static debate-style engagements among LLMs for enhanced reasoning . Liu et al. (2023) enables agents to interact for multiple rounds in a dynamic architecture. Li et al. (2023a); Hong et al. (2023); Wu et al. (2023); Chen et al. (2023c;a) offer several multi-agent frameworks that enable the development of LLM applications or enhance task-solving capabilities. However, these methods primarily focus on the interaction structures between LLM agents, rather than the relationship between the number of agents and performance. We also select representative methods Du et al. (2023); Shinn et al. (2023) to combine with our method, achieving further enhancements. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3 Method ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In this section, we introduce Agent Forest, which is implemented through a two-phase process: sampling and voting. The overview of our method is shown in Figure 2. ",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/c8e5cc0b25eeb2fe3f857a1293b42b555807a0fc1551a8b15da511724e38f806.jpg",
        "image_caption": [
            "Figure 2: Illustration of Agent Forest. The two-phase process begins by feeding the task query, either alone or combined with prompt engineering methods, into LLM agents to generate answers. Subsequently, majority voting is applied to these answers to determine the final answer. Specifically, an LLM agent refers to a single LLM or a multiple LLM-Agents collaboration framework. "
        ],
        "image_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Algorithm 1 Agent Forest ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Require: Query $x$ , number of samples $N$ , LLM $\\mathcal { M }$ or LLM integrated with other methods $f _ { \\mathcal { M } } ( x )$   \n1: Initialize an empty set for samples $S \\gets \\emptyset$   \n2: for $i = 1$ to $N$ do   \n3: Generate sample $s _ { i } \\gets \\mathcal { M } ( x )$ or $s _ { i } \\gets f _ { \\mathcal { M } } ( x )$   \n4: Add sample to the set $S \\gets S \\cup \\{ s _ { i } \\}$   \n5: end for   \n6: for each sample $s _ { i }$ in $S$ do   \n7: Initialize similarity scores $V ( s _ { i } ) \\gets 0$ (id:)   \n8: for each sample $s _ { j }$ in $S$ do   \n9: if $i \\neq j$ then   \n10: $V ( s _ { i } ) \\gets V ( s _ { i } ) + s i m ( s _ { i } , s _ { j } )$   \n11: end if   \n12: end for   \n13: end for   \n14: $A \\gets \\arg \\operatorname* { m a x } _ { s _ { i } \\in S } V ( s _ { i } )$   \n15: return $A$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Sampling. Let $x$ represent the task query and $\\mathcal { M }$ denote an LLM. In this phase, we generate $N$ samples by solely querying the LLM $\\textit { \\textbf { M N } }$ times with each sample represented as $s = \\mathcal { M } ( x )$ or by integrating with other methods $f _ { \\mathcal { M } }$ with $N$ times executions where each sample is denoted as $s = f _ { \\mathcal { M } } ( x )$ . We obtain a set of samples ${ \\cal S } = \\{ s _ { 1 } , s _ { 2 } , . . . , s _ { N } \\}$ at the end of this phase. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Voting. Let $A$ represent the final answer. In this phase, we employ majority voting to consolidate the response sample set $S$ into the final answer $A$ . This involves calculating the cumulative similarity for each sample relative to the others, denoted as $\\begin{array} { r } { V ( s _ { i } ) = \\sum _ { j = 1 , j \\neq i } ^ { N } s i m ( s _ { i } , s _ { j } ) } \\end{array}$ . For open-ended generation tasks such as code generation, the BLEU score proposed by Papineni et al. (2002) is utilized to quantify similarity. Conversely, for close-ended tasks like multiple-choice questions, similarity is measured by occurrence frequency. The sample that exhibits the highest cumulative similarity is then chosen as the final answer denoted as $A = \\arg \\operatorname* { m a x } _ { s _ { i } \\in S } V ( s _ { i } )$ . ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The complete process of Agent Forest is described in Algorithm 1. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4 Experimental Setup ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We separate the experimental setup (this section) with evaluations (next section), to introduce the coverage of scenarios/tasks compared with the most related works (for examining the comprehensiveness of our work), the backbone language models we adopted (for examining the applicability of our work), and the methods combined with ours (for examining the compatibility and orthogonality of our work). ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Tasks Our method is evaluated on the following task: ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Arithmetic Reasoning. Similar to Wang et al. (2023b); Fu et al. (2023); Du et al. (2023), we select the GSM8K Cobbe et al. (2021a) as one of the test sets. Additionally, we select the more challenging MATH dataset Hendrycks et al. (2021b), which is used by Wu et al. (2023).   \n• General Reasoning. Similar to Du et al. (2023); Jiang et al. (2023), we select the MMLU Hendrycks et al. (2021a). Additionally, we select the dataset from the chess state tracking task (Chess) 1, which is used by Du et al. (2023); Zhang et al. (2023). Code Generation. Similar to Liu et al. (2023), we select the HumanEval Chen et al. (2021). To implement our method, we compute the BLEU score Papineni et al. (2002) among all pairs of generated candidate answers. The answer with the highest cumulative BLEU score is then selected as the final output. ",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/0a260c570a69d1d8be725ba3d9cb1b941e7d7f71972de7a0a29026e4999ee506.jpg",
        "table_caption": [
            "Table 1: Comparing the conducted experiments with the most related works. Our comprehensive study encompasses various LLMs, multiple tasks, and the integration with multiple methods. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"3\">Methods</td><td rowspan=\"3\">Various LLMs</td><td colspan=\"3\">Tasks</td><td colspan=\"3\">Integrated with Methods</td></tr><tr><td>Chat</td><td>Arithometii</td><td>Cenonn</td><td>GeCeratio</td><td>EPpompt</td><td>Miltiple IM aents</td></tr><tr><td>CoT-SC Wang et al. (2023b)</td><td></td><td></td><td></td><td></td><td></td><td>Only CoT Wei et al. (2022)</td><td></td></tr><tr><td>Complexity-CoT Fu et al. (2023)</td><td>√ ✓</td><td></td><td>✓</td><td>✓ ✓</td><td></td><td>Only CoT Wei et al. (2022)</td><td></td></tr><tr><td>Debate Du et al. (2023) Blended Lu et al. (2024)</td><td>√</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>√</td><td></td><td>√</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Language models adopted We evaluate our method using language models of different scales from the Llama2 Touvron et al. (2023) and GPT series OpenAI (2022). Specifically, we evaluate two versions of Llama2-Chat2, optimized for conversational use cases through alignment techniques, with model sizes of 13B and 70B parameters. Additionally, we include GPT-3.5-Turbo and GPT-4 in our evaluation. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Methods enhanced by our method To examine the comparability of our method, we study the integration of various typical methods from two distinct categories with our method: ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Prompt Engineering. Various prompt engineering methods are considered to conduct comprehensive experiments. We evaluate Chain-of-Thought prompting (CoT) Wei et al. (2022), Zero-Shot Chain-ofThought prompting (Zero-Shot Cot) Kojima et al. (2022), and more sophisticated methods such as Solo Performance Prompting (SPP) Wang et al. (2023c). Initially, these methods are applied with a single LLM query. We then increase the number of queries and employ majority voting to determine the most consistent answer as the final response.   \n• Multiple LLM Agents Collaboration. We select LLM-Debate Du et al. (2023) denoted as Debate, and self-reflection Shinn et al. (2023) denoted as Reflection. Within these methods, we generate multiple samples by iteratively operating these methods and using majority voting to produce the final answer. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Specifically, the effectiveness of our method is evaluated by averaging the results across 10 independent runs. During each run, we scale up the ensemble size to 40 to ensure maximum gains. However, when integrating our method with the Debate Du et al. (2023), the ensemble size is limited to 10 due to the significant computational overhead introduced by the communication architecture. Detailed experimental settings are provided in the Appendix A. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5 Experimental Results ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/26f879654e9b91296ec7e66eb5a7af87d5a58e8c9c3ff68270d44b29e1e80cc5.jpg",
        "image_caption": [
            "Figure 3: The accuracy scales with the ensemble size of our method across different tasks with various LLMs. The error bars represent the standard error. "
        ],
        "image_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table 2: Our method generally enhances performance across all tasks and LLMs. The bolded instances indicate that smaller LLMs outperform the larger LLMs. “Single” denotes that the LLM is queried only once. GPT-4 is used only for comparison with other methods, hence it only presents “Single” results. “Ours” denotes our method where the ensemble size is 40. The error bars represent the standard error. ",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/efe7f5188b07da63073bee9f42b67863b8eb81f5bdfc015d5c461122d9511a9a.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">GSM8K</td><td colspan=\"2\">MATH</td><td colspan=\"2\">Chess</td><td colspan=\"2\">MMLU</td><td colspan=\"2\">HumanEval</td></tr><tr><td>Single</td><td>Ours</td><td>Single</td><td>Ours</td><td>Single</td><td>Ours</td><td>Single</td><td>Ours</td><td>Single</td><td>Ours</td></tr><tr><td>Llama2-13B Touvron et al. (2023)</td><td>0.35 ± 3e-2</td><td>0.59 ± 5e-4</td><td>0.03± 7e-3</td><td>0.09 ± 2e-3</td><td>0.14 ± 2e-2</td><td>0.18± 2e-3</td><td>0.42 ± 3e-2</td><td>0.51 ± 1e-3</td><td>0.14 ± 1e-2</td><td>0.18 ± 1e-3</td></tr><tr><td>Llama2-70B Touvron et al. (2023)</td><td>0.54 ± 3e-2</td><td>0.74± 1e-3</td><td>0.05 ± 1e-2</td><td>0.11 ± 1e-3</td><td>0.12 ± 2e-2</td><td>0.13 ± 2e-3</td><td>0.55 ± 2e-2</td><td>0.60± 3e-3</td><td>0.24 ± 1e-2 0.33 ± 1e-3</td><td></td></tr><tr><td>GPT-3.5-Turbo OpenAI (2022)</td><td>0.73 ± 2e-2</td><td>0.85± 3e-3</td><td>0.29 ± 2e-2</td><td>0.39± 2e-3</td><td>0.51 ± 3e-2</td><td>0.55 ± 2e-3</td><td>0.59 ± 3e-2</td><td>0.70± 2e-3</td><td>0.67 ± 2e-2</td><td>0.73± 1e-2</td></tr><tr><td>GPT-4 OpenAI (2022)</td><td>0.88 ± 2e-2</td><td></td><td>0.40 ± 3e-2</td><td></td><td>0.65 ± 2e-2</td><td></td><td>0.77±2e-2</td><td></td><td>0.88 ± 3e-2</td><td></td></tr></table>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5.1 Generalizability ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table 2 and Figure 3 show that our method generally enhances performance across all tasks and LLMs by increasing the ensemble size. Specifically, in arithmetic reasoning tasks, the accuracy gains range from $1 2 \\%$ to $2 4 \\%$ on the GSM8K and from 6% to $1 0 \\%$ on the MATH. In general reasoning tasks, the accuracy gains range from $1 \\%$ to $4 \\%$ on the Chess and from 5% to 11% on the MMLU. In code generation task, the accuracy gains range from 4% to 9% on HumanEval. Surprisingly, our method enables a smaller LLM to outperform a larger counterpart by simply scaling up the ensemble size. For instance, the enhanced Llama2-13B model achieves 59% accuracy on the GSM8K dataset, outperforming the Llama2-70B model, which scores $5 4 \\%$ Additional statistical results are presented in Appendix B.2. ",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/06f33a5d4e163216800f72c3983c2c9488b7615f4c2009c9875658fbe4ac34b6.jpg",
        "table_caption": [
            "Table 3: Our method outperforms other methods used standalone in most cases and always enhances other methods across various tasks and LLMs. The bolded instances indicate the highest accuracy for each task and the underlined instances indicate the highest accuracy in standalone cases. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8K</td><td colspan=\"2\">MATH</td><td colspan=\"2\">Chess</td><td colspan=\"2\">MMLU</td><td colspan=\"2\">HumanEval</td></tr><tr><td>Standalone</td><td>+Ours</td><td>Standalone</td><td>+Ours</td><td>Standalone</td><td>+Ours</td><td>Standalone</td><td>+Ours</td><td>Standalone</td><td>+Ours</td></tr><tr><td rowspan=\"6\">Llama2-13B</td><td>COT Wei et al. (2022)</td><td>0.39</td><td>0.56 (+0.17)</td><td>0.04</td><td>0.06 (+0.02)</td><td>0.18</td><td>0.23 (+0.07))</td><td>0.42</td><td>0.43 (+0.01)</td><td>0.13</td><td>0.20 (+0.07)</td></tr><tr><td>ZS-COT Kojima et al. (2022)</td><td>0.40</td><td>0.61 (+0.21)</td><td>0.03</td><td>0.08 (+0.05)</td><td>0.15</td><td>0.20 (+0.05)</td><td>0.42</td><td>0.48 (+0.06)</td><td>0.15</td><td>0.22 +0.07)7)</td></tr><tr><td>SPP Wang et al. (2023)</td><td>0.19</td><td>0.42 (+0.23)</td><td>0.01</td><td>0.04 (+0.03)</td><td>0.21</td><td>0.26 (+0.05)</td><td>0.32</td><td>0.53 (+0.21)</td><td>0.03</td><td>0.08 (+0.05)</td></tr><tr><td>Touvron et al. (2023) Debate Du et al. (2023)</td><td>0.38</td><td>0.48 (+0.10)</td><td>0.05</td><td>0.07(+0.02)</td><td>0.18</td><td>0.19 (+0.01)</td><td>0.37</td><td>0.39 (+0.02)</td><td></td><td>0</td></tr><tr><td>Refection Shinn et al. (2023)</td><td>0.36</td><td>0.59 (+0.23)</td><td>0.01</td><td>0.03 (+0.02)</td><td>0.13</td><td>0.19 (+0.06)</td><td>0.45</td><td>0.50 (+0.05)</td><td>0.06</td><td>0.13 (+0.07)</td></tr><tr><td>Ours</td><td>0.59</td><td></td><td>0.09</td><td></td><td>0.18</td><td></td><td>0.51</td><td></td><td>0.25</td><td></td></tr><tr><td rowspan=\"6\">Llama2-70B</td><td>COT Wei et al. (2022)</td><td>0.57</td><td>0.72 (+0.1)</td><td>0.06</td><td>0.13 (+0.07)</td><td>0.10</td><td>0.11 (+0.01)</td><td>0.56</td><td>0.57 (+0.01)</td><td>0.30</td><td>0.32 (+0.02)</td></tr><tr><td>ZS-COT Kojima et al. (2022)</td><td>0.57</td><td>0.73 (+0.16)</td><td>0.04</td><td>0.10 (+0.06)</td><td>0.20</td><td>0.27 (+0.07)</td><td>0.54</td><td>0.65 (+0.1)</td><td>0.23</td><td>0.29 (+0.06)</td></tr><tr><td>SPP Wang et al. (2023c)</td><td>0.42</td><td>0.69 (+0.27)</td><td>0.03</td><td>0.09 (+0.06)</td><td>0.16</td><td>0.27 (+0.11))</td><td>0.49</td><td>0.63 (+0.14)</td><td>0.15</td><td>0.20 (+0.05</td></tr><tr><td>Touvron et al. (2023) Debate Du et al. (2023)</td><td>0.59</td><td>0.65 (+0.06)</td><td>0.10</td><td>0.11 (+0.01)</td><td>0.14</td><td>0.17 (+0.03)</td><td>0.56</td><td>0.58 (+0.02)</td><td>0</td><td>0</td></tr><tr><td>Refection Shinn et al. (2023)</td><td>0.52</td><td>0.77 (+0.25)</td><td>0.02</td><td>0.05 (+0.03)</td><td>0.15</td><td>0.26 (+0.11)</td><td>0.42</td><td>0.55 (+0.13)</td><td>0.16</td><td>0.26 (+0.10)</td></tr><tr><td>Ours</td><td>0.74</td><td></td><td>0.11</td><td></td><td>0.13</td><td></td><td>0.60</td><td></td><td>0.33</td><td></td></tr><tr><td rowspan=\"7\">GPT-3.5-Turbo OpenAI (2022)</td><td>COT Wei et al. (2022)</td><td>0.74</td><td>0.84 (+0.10)</td><td>0.28</td><td>0.41 (+0.13)</td><td>0.50</td><td>0.55 (+0.05))</td><td>0.61</td><td>0.64 (+0.03)</td><td>0.70</td><td>0.75 (+0.05)</td></tr><tr><td>ZS-COT Kojima et al. (202)</td><td>0.74</td><td>0.88 (+0.14)</td><td>0.25</td><td>0.40 (+0.15)</td><td>0.35</td><td>0.48 (+0.13)</td><td>0.58</td><td>0.69 (+0.11)</td><td>0.67</td><td>0.74 (+0.07)</td></tr><tr><td>SPP Wang et al. (202c)(d</td><td>0.70</td><td>0.83 (+0.13)</td><td>0.26</td><td>0.39 (+0.13)</td><td>0.37</td><td>0.54 (+0.17)</td><td>0.53</td><td>0.68 (+0.1)</td><td>0.57</td><td>0.64 (+0.07)</td></tr><tr><td>Debate Du et al. (2023)</td><td>0.83</td><td>0.85 (+0.02)</td><td>0.32</td><td>0.36 (+0.04)</td><td>0.49</td><td>0.57(+0.08</td><td>0.56</td><td>0.67 (+0.1)</td><td>0.18</td><td>0.24 (+0.06)</td></tr><tr><td>Refection Shinn et al. (2023)</td><td>0.76</td><td>0.84 (+0.08)</td><td>0.27</td><td>0.41 (+0.14)</td><td>0.44</td><td>0.57 (+0.13)</td><td>0.39</td><td>0.44 (+0.05)</td><td>0.58</td><td>0.73 (+0.15)</td></tr><tr><td>Ours</td><td></td><td></td><td></td><td></td><td>0.55</td><td></td><td>0.70</td><td></td><td>0.73</td><td></td></tr><tr><td></td><td>0.85</td><td></td><td>0.39</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/587dd36ef1500a5d204b876a42caf0a202a9ceec14c17d7753338d25e0c67b2c.jpg",
        "image_caption": [
            "Figure 4: Our method improves accuracy over various hyperparameters and tasks. The default $T$ is 1.0 and the default $p$ is 1.0. "
        ],
        "image_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.2 Compatibility ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Table 3 shows that by integrating our method with other methods, the performance can be further improved across different LLMs and tasks, despite these methods have different implementations. To be specific, in arithmetic reasoning tasks, our method enhances these methods to further improvement, yielding increases between $1 0 \\%$ and $2 1 \\%$ on the GSM8K dataset, and between $1 \\%$ and 15% on the MATH dataset. In general reasoning tasks, integration with other methods generally achieves performance gains ranging from $1 \\%$ to $1 3 \\%$ in the Chess task and from $1 \\%$ to $1 1 \\%$ in the MMLU task. In code generation task, when combined with other methods, gains range from 2% to 7%. However, two notable exceptions are observed when integrated with the debate method with the Llama2-13B and Llama2-70B models, which result in failed cases. This failure in performance is attributed primarily to the noise generated by referencing the answers of other agents during the debate process. The synthesized responses, which incorporate input from multiple agents, disrupt the coherence of the code logic, leading to the observed performance degradation. All accuracy curves are provided in the Appendix B.1. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "5.3 Effectiveness ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "From Table 3, we find that our method outperforms other methods in standalone cases, except on the Chess task using Llama2-13B and Llama2-70B. Additionally, based on the data from Table 3, we have calculated the average performance ranking of each enhanced method across various tasks, with the results presented in ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Table 4. Notably, without the need for additional prompts or complex LLM collaboration frameworks, our method achieves the highest average ranking across different LLMs and tasks. ",
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/e2bde748f0215a7728a9b65827396fe2a99e1740aa22a30451dfa5657828b7a3.jpg",
        "table_caption": [
            "Table 4: Our method achieved the highest average ranking across different LLMs and tasks. Rankings are derived from Table 3 and are based on the average rank each method achieves across all five tasks for a given LLM. The bolded instances indicate the top ranking. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Method +Ours</td><td>GPT-3.5</td><td>70B</td><td>13B</td><td>Overall</td></tr><tr><td>COT Wei et al. (2022)</td><td>2.8</td><td>3.6</td><td>3.6</td><td>3.3</td></tr><tr><td>ZS-COT Kojima et al. (2022)</td><td>2.8</td><td>2.4</td><td>3</td><td>2.7</td></tr><tr><td>SPP Wang et al. (2023c)</td><td>4.6</td><td>3.6</td><td>3.8</td><td>4</td></tr><tr><td>Debate Du et al. (2023)</td><td>3.8</td><td>4.4</td><td>5</td><td>4.4</td></tr><tr><td>Refection Shinn et al. (2023)</td><td>3</td><td>4.0</td><td>3</td><td>3.3</td></tr><tr><td>Ours</td><td>2.6</td><td>2.6</td><td>2.2</td><td>2.5</td></tr></table>",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "5.4 Robustness ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "We conduct ablation studies to evaluate the impact of changes in various hyperparameters on the final performance. The experiment is conducted by altering the temperature $T$ Ficler $\\&$ Goldberg (2017) and the nucleus probability $p$ Radford et al. (2019), using the GPT-3.5-Turbo model over an average of 20 runs. As shown in Figure 4, scaling up ensemble size improves the LLM performance consistently across different tasks, despite the variation of these hyperparameters. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "5.5 Token usage ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "We record the token usage for different methods, with the Table 5 presenting the token usage for a single agent. Given that our method scales up by increasing the ensemble size, the token usage increases proportionally with the number of agents or when combined with other methods. When addressing specific tasks, one can trade a higher token budget for improved performance. More details are presented in Appendix B.3 ",
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/990f1c2d487dad795b557e71fb30fc3bab617a568c83fcd606da92d00ec0db43.jpg",
        "table_caption": [
            "Table 5: Token usage of GPT-3.5 "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Method</td><td>GSM8K</td><td>MATH</td><td>Chess</td><td>MMLU</td><td>HumanEval</td></tr><tr><td>Ours (single agent)</td><td>235± 54</td><td>326 ± 131</td><td>138 ± 14</td><td>247 ± 91</td><td>495 ± 120</td></tr><tr><td>COT Wei et al. (2022)</td><td>261 ± 63</td><td>360 ± 134</td><td>284±76</td><td>330 ± 110</td><td>330 ± 116</td></tr><tr><td>ZS-COT Kojima et al. (2022)</td><td>228± 56</td><td>305± 122</td><td>132 ± 12</td><td>230± 92</td><td>305± 132</td></tr><tr><td>SPP Wang et al. (2023c)</td><td>341 ± 85</td><td>471 ± 161</td><td>363± 35</td><td>428± 123</td><td>501± 125</td></tr><tr><td>Reffection Shinn et al. (2023)</td><td>214±45</td><td>281±96</td><td>146± 13</td><td>218±80</td><td>338± 110</td></tr></table>",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "6 Understanding the Performance Gains ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Table 2 shows that the efficacy of our method varies with the difficulty of the task. In this section, we aim to understand the underlying properties through controlled experiments. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "To start the analysis, we select two datasets with increasing difficulty, i.e., GSM8K and MATH, to calculate the relative performance gain. The relative performance gain $\\eta$ is given by: $\\begin{array} { r } { \\eta = \\frac { { P _ { \\mathrm { m } } - P _ { \\mathrm { s } } } } { { P _ { \\mathrm { s } } } } } \\end{array}$ where $P _ { \\mathrm { m } }$ and $P _ { \\mathrm { { s } } }$ are the performances (accuracy) with our method and a single LLM query, respectively. The results are shown in Table 6. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "It is noteworthy that the relative performance gain is more substantial with increasing task difficulty. Specifically, we observe that within the same task, the smaller model, Llama2-13B, gains ranging from 28%-200%, but only 8%-16% over GPT-3.5-Turbo. Moreover, the more challenging task MATH yields gains of 34%-200%, in contrast to only 16%-69% on the easier task GSM8K. ",
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/327f8cc716607f52d8063486dc30f00ef18a0d1085b7131dcb76bf756db321e9.jpg",
        "table_caption": [
            "Table 6: The relative performance gain ( $\\%$ ) becomes more significant when the relative difficulty between the LLM and the task increases. It is calculated based on Table 2. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Task</td><td>Llama2-13B</td><td>Llama2-70B</td><td>GPT-3.5-Turbo</td></tr><tr><td>GSM8K (easy)</td><td>69</td><td>37</td><td>16</td></tr><tr><td>MATH (hard)</td><td>200</td><td>120</td><td>34</td></tr></table>",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To further analyze this correlation in detail, we categorize the difficulty of a given task into three orthogonal dimensions: 1) the inherent difficulty of the task; 2) the number of steps required to solve the task; 3) the prior probability of the correct answer. To investigate these dimensions, we conduct experiments that can isolate each dimension. And then, we delve into each dimension in detail. ",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/38284d17a1e2183720a7871ebece73d76b69c03941d4c0bb0c90c4bc23bde827.jpg",
        "image_caption": [
            "Figure 5: Illustration of three dimensions for a given task. Nodes represent steps, while dashed lines indicate alternative potential steps. The depth of nodes represents the number of steps, and the color intensity represents the level of inherent difficulty. "
        ],
        "image_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6.1 Isolation ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To explicitly explore the impact of these dimensions, we conduct a mathematical task designed to isolate each one. Consider the task detailed below: ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/10130da90081af085fabbe2871eb21c38a1f2b0454b75f51346f782a3d564112.jpg",
        "text": "$$\n{ \\mathrm { F i n d ~ t h e ~ i n t e r v a l ~ } } \\Delta _ { k } { \\mathrm { ~ s u c h ~ t h a t ~ } } \\sum _ { i = 1 } ^ { S } a _ { i } \\cdot b _ { i } \\in \\Delta _ { k } ,\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where: ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "• $a _ { i } , b _ { i }$ are randomly chosen integers from the closed interval $[ - I , I ]$ . $I \\in \\mathbb { Z } ^ { + }$ defines the range of integers. $I$ represents the inherent difficulty of the question. A larger value of $I$ indicates a more challenging task.   \n• $S \\in \\mathbb { Z } ^ { + }$ is the number of terms in the summation. $S$ represents the number of steps required to solve the problem. A larger value of $S$ indicates a more challenging task.   \n• The result space is partitioned into $K$ intervals $\\Delta _ { 1 } , \\Delta _ { 2 } , \\ldots , \\Delta _ { K }$ of equal probability. $K \\in \\mathbb { Z } ^ { + }$ denotes the number of these intervals. $1 / K$ represents the prior probability of the correct answer. A lower prior probability indicates a more challenging task. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In the following experiments, we analyze each dimension respectively based on GPT-3.5-Turbo. Note that we use GPT-3.5-Turbo for a case study, it can also be changed to other backbone models. The relative performance gains are measured by the difference between the maximum accuracy our method can achieve (sampling 40 times) and the accuracy of a single LLM query (sample once). Results are averaged over 10 runs. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "6.2 Inherent Difficulty ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Property 1: Gains increase then decrease by rising the inherent difficulty. We investigate the inherent difficulty by varying $I$ from 10 to 400, while keeping the values of $S$ and $K$ constant across four groups of different values, from small to large, respectively. Figure 6 (left) shows an initial uptick in performance gains with increases in $I$ , indicating that our method can significantly enhance performance in line with rising inherent difficulty. The most notable gains are seen at $I = 1 0 0$ and $I = 2 0 0$ , consistent across all $S$ and $K$ settings. Yet, at $I = 4 0 0$ , gains taper off, implying that excessive complexity may exceed the model’s reasoning capabilities, leading to diminishing returns for our method under extreme task difficulty. ",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/ada2eb1b6374d8e9db83a3384f1cdd3895a8900ab0b28fa1d63daf932278e0d7.jpg",
        "image_caption": [
            "Figure 6: (Left) The relative performance gains increase and then decrease with rising inherent difficulty. (Middle) The relative performance gains increase with the number of steps. (Right) The absolute performance increases with the prior probability. We analyze each dimension by fixing the other two dimensions. The error bars represent the standard error. "
        ],
        "image_footnote": [],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "6.3 Number of Steps ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Property 2.1: Gains increase with the number of steps. We analyze the number of steps by isolating $S$ We tune $S$ from 1 to 8, while keeping the values of $I$ and $K$ constant across four groups of different values, ranging from small to large, respectively. Figure 6 (middle) shows that as the number of steps increases, there is a corresponding increase in performance gain. Additionally, we find that when $I$ and $K$ are increased (which indicates a higher difficulty), the performance gains are more significant, e.g., 4%-18% gains over $\\{ I = 1 0 , K = 2 \\}$ compared to $1 6 \\%$ -48% over $\\{ I = 1 0 0 , K = 4 \\}$ . ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Property 2.2: Agent Forest increases the performance for each step. We conduct a fine-grained analysis for each step of a given task. We explicitly prompt the language model to output the result of each step. Subsequently, we utilize Agent Forest at each step to derive the answer for that step. Figure 7 (left) shows that although each step has equal inherent difficulty, the accumulation of errors from previous steps lead to a decrease in accuracy as the number of steps increases. However, our method mitigates the performance decrease encountered with increasing steps. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Derivation. Based on Property 2, we propose Step-wise Agent Forest, which can further enhance the performance. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Step-wise Agent Forest initially prompts the LLM to decompose the task into multiple steps. It then proceeds with multi-round iterations to produce the final result. In each round, the process begins by selecting a current unprocessed step and using Agent Forest to determine the result of that step. Subsequently, it uses ",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/fbd45818e5a5c5e9151adcc9d7cd5e75495b9476f7ee1e00daa9fb7326fcb14b.jpg",
        "image_caption": [
            "Figure 7: (Left) Our method increases the performance for each step. Blue bars show the accuracy of various steps for a single sample, and orange bars show the gains for 40 samples. (Middle) Step-wise Agent Forest can further enhance the performance across different levels of inherent difficulty. (Right) Hierarchical Agent Forest can further enhance the performance with homogeneous and heterogeneous model combinations. "
        ],
        "image_footnote": [],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "the result to update the task. This iterative process is repeated multiple times until the last step is processed.   \nTo evaluate the performance of Step-wise Agent Forest, we fix $S = 8$ and $K = 4$ , and tune $I$ from 100 to 400.   \nFigure 7 (middle) shows that compared to Agent Forest, Step-wise Agent Forest yields greater improvements.   \ne.g., we see 15%-42% gains, which increase with inherent difficulty. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "6.4 Prior Probability ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Property 3: The performance increases with the prior probability. We investigate the influence of prior probability on performance by tuning the parameter $K$ , while maintaining constant values for $I$ and $K$ . As $K$ represents the number of intervals, the prior probability is defined as $1 / K$ . We vary $K$ from 4 to 32, which equivalently alters the prior probability from $1 / 4$ to $1 / 3 2$ . Through four experimental groups illustrated in Figure 6 (right), each characterized by different configurations of $I$ and $S$ , we find that as the prior probability increases, so does the performance. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Derivation. Based on Property 3, we propose Hierarchical Agent Forest can further enhance the performance. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "As the performance is related to the prior probability, decomposing low-probability tasks into multiple high-probability subtasks and addressing them hierarchically can boost performance. Moreover, subtasks with varying prior probabilities can be addressed using different models. Additionally, cost savings can be achieved by using simpler, less expensive models for the easier, higher-probability subtasks. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In our experiments, the task is to solve the problem with $K = 3 2$ . GPT-3.5-Turbo is used in homogeneous combination experiment and GPT-3.5-Turbo and GPT-4 are used in heterogeneous combination experiment. The results are presented in Figure 7 (right). ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In homogeneous combination experiment, by employing the hierarchical method, we start with $K = 8$ to obtain an intermediate answer and then find the solution with $K = 3 2$ , focusing on intervals identified by the intermediate answer. This method enhances the performance from $2 1 \\%$ to $3 1 \\%$ , demonstrating that the hierarchical method can further enhance the performance. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In heterogeneous combination experiment, GPT-3.5-Turbo is used for generating the intermediate answer with $K = 8$ , and GPT-4 is then employed to solve for the final answer with $K = 3 2$ . In Figure 7 (right), compared with the result of GPT-4 with $K = 3 2$ , the hierarchical method improves performance from $3 5 \\%$ to 47%, suggesting the deployment of different LLMs at the corresponding level of problem-solving can improve the performance in a cost-effective manner. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "7 Conclusions and Future Work ",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "In this paper, we report that more agents is all you need, i.e., simply adding more instantiated LLM agents is what you need to obtain a better LLM performance in processing complex tasks, without bothering complicated methods, such as CoT pipelines, multi-agent collaboration frameworks, etc. We have conducted the first comprehensive study in the literature to understand such a “scaling law”, including when it holds and how to facilitate its occurrence. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "The results indicate that Agent Forest can generally improve the performance of LLMs by increasing the ensemble size. Importantly, this method is orthogonal to different existing methods, which can lead to further improvements when combined with them. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Furthermore, we observe that the performance gains are influenced by the difficulty of the task. To explore this correlation, we isolate and analyze three dimensions of task difficulty: the inherent difficulty, the length of reasoning steps, and the prior probability of a correct answer. We find that: 1) the performance gains increase then decrease by rising the inherent difficulty; 2) performance gains increase with the number of steps; and 3) performance increases with the prior probability. Based on these properties, we also develop ways to boost the effectiveness of “More Agents”. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Considering that each input remains the same when we increase the number of agents, the sampling phase can be optimized to reduce the cost. Nonetheless, such a challenge of escalating costs commonly exists in works requiring multiple LLM calls Wang et al. (2023b); Du et al. (2023). This aligns with recent findings as discussed in Kapoor et al. (2024) that current benchmarks focus narrowly on accuracy, leading to costly agents. Additionally, the lack of adequate holdout sets and standardization in evaluation practices further complicates the development of cost-effective agents. Addressing these issues can enhance the practical efficiency of AI agents in real-world scenarios. We leave it as a future work to optimize. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Leo Breiman. Random forests. Machine learning, 45:5–32, 2001. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F. Karlsson, Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation. CoRR, abs/2309.17288, 2023a.   \nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. CoRR, abs/2305.05176, 2023b.   \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.   \nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. CoRR, abs/2308.10848, 2023c.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021a.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021b.   \nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. CoRR, abs/2305.14325, 2023.   \nJessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. CoRR, abs/1707.02633, 2017.   \nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a.   \nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b.   \nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. CoRR, abs/2308.00352, 2023.   \nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 14165–14178. Association for Computational Linguistics, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, and Arvind Narayanan. Ai agents that matter. arXiv preprint arXiv:2407.01502, 2024. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: communicative agents for \"mind\" exploration of large scale language model society. CoRR, abs/2303.17760, 2023a. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315–5333, Toronto, Canada, July 2023b. Association for Computational Linguistics. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. CoRR, abs/2305.19118, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Lei Lin, Jia-Yi Fu, Pengli Liu, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, and Kun Gai. Ask one more time: Self-agreement improves reasoning of language models in (almost) all scenarios. CoRR, abs/2311.08154, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. CoRR, abs/2310.02170, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models. CoRR, abs/2311.08692, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, and William Beauchamp. Blending is all you need: Cheaper, better alternative to trillion-parameters llm. arXiv preprint arXiv:2401.02994, 2024. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL https://openai.com/blog/ chatgpt. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40st Annual Meeting of the Association for Computational Linguistics ACL 2002, Philadelphia, USA, July, 2002, pp. 311–318. Association for Computational Linguistics, 2002. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and Mikhail Yurochkin. Large language model routing with benchmark datasets. CoRR, abs/2309.15789, 2023. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agüera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.   \nFanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. arXiv preprint arXiv:2401.10491, 2024.   \nHongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, and Mikhail Yurochkin. Fusing models with complementary expertise. CoRR, abs/2310.01542, 2023a.   \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b.   \nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. CoRR, abs/2307.05300, 2023c.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.   \nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023.   \nKai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining the inter-consistency of large language models: An in-depth analysis via debate. CoRR, abs/2305.11595, 2023.   \nJintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124, 2023.   \nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "A Detailed Experiment Settings ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A.1 Common Settings ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In all experiments involving GPT-3.5-Turbo presented in Section 4, we utilize the model version gpt-3.5-turbo-0613. In Table 2, the notation GPT-4 corresponds to the model version gpt-4-0613. For the experiments conducted with GPT-3.5-Turbo in Section 6, we employ the model version gpt-3.5-turbo-1106 with the JSON mode enabled. Similarly, GPT-4 in this context refers to gpt4-1106-Preview operating in JSON mode. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A.2 Experiments on Arithmetic Reasoning Tasks ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "For the implementation of Agent Forest to arithmetic reasoning tasks within the GSM8K and MATH datasets, we execute the initial sampling phase by Algorithm 1. Samples are extracted from the responses by matching “boxed $\\{ \\{ \\mathrm { X } \\} \\}$ ”, where “X” denotes a numerical value or a mathematical expression. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In the subsequent voting phase, to evaluate the similarity between samples, as outlined in Algorithm 1, we employ mathematical equivalence comparisons for each sample. The most probable sample is chosen as the final answer. This answer is subjected to a comparison of mathematical equivalence with the ground truth to ascertain the correctness of the result. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A.3 Experiments on General Reasoning Tasks ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "For general reasoning tasks, as encountered in the MMLU and Chess datasets, Agent Forest is applied following Algorithm 1 during the sampling phase. Samples are extracted by matching the pattern “(X” or “(X)”, where “X” corresponds to the options A, B, C, or D in MMLU task and the chessboard position in Chess task. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "During the voting phase, we calculate similarity by counting the frequency of each option’s occurrence within the samples. The most frequently occurring option is then chosen as the final answer. This selected answer is compared with the ground truth to determine the accuracy of the result. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "A.4 Experiments on Code Generation Task ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In the code generation task, we apply the Agent Forest method to produce Python code using the HumanEval dataset. During the sampling phase, we extract Python code snippets from the model’s responses. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In the voting phase, we compute the BLEU score using sacreBLEU Post (2018) to evaluate the similarity between each of the generated samples. The sample with the highest cumulative BLEU score is selected as the final answer. This method ensures that the final output is the most representative and accurate piece of code as determined by consensus through similarity scoring among the samples. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "B Detailed Experiment Results ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "B.1 Additional Accuracy Curves ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In this section, we provide the accuracy curves of our experiments across various datasets when utilizing different LLMs. From these curves, we demonstrate that our method has the following properties: ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• Generalizability. By using our method standalone, the performance can be generally enhanced by increasing the ensemble size. • Compatibility. Our method can generally enhance other methods by increasing the ensemble size. ",
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/34c18f4e234d1f32108f7384ace1c3642ba5a1f0c6986beb11576c433571b3d4.jpg",
        "image_caption": [
            "Figure 8: Accuracy curves across various datasets using the Llama2-13B model. "
        ],
        "image_footnote": [],
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/da1bd8e82e4d4157790478f0be0574af4d7c19289ab1e9a3802a527df2e0cab9.jpg",
        "image_caption": [
            "Figure 9: Accuracy curves across various datasets using the Llama2-70B model. "
        ],
        "image_footnote": [],
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/bbf699a2a55fdb7c09898b8769bb65bd6bdca33874b9e96163b538d6c78d7455.jpg",
        "image_caption": [
            "Figure 10: Accuracy curves across various datasets using the GPT-3.5-Turbo model. "
        ],
        "image_footnote": [],
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/5a1d2496fba54d6072ee92cb6f4720fe0716b1fb26fd856566c83f5f98712c80.jpg",
        "image_caption": [
            "Figure 11: Debate accuracy curves across various datasets using the Llama2-13B model. "
        ],
        "image_footnote": [],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "B.2 Statistical Test Results ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "In this section, we performed a one-way ANOVA to determine if there are significant differences in accuracy across different ensemble sizes (1, 10, 20, 30, 40), across 10 independent runs. The detailed p-values are provided in Table 7, where all p-values are less than 0.05, demonstrating that the differences in accuracy between ensemble sizes are significant. ",
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/e3521c65338c8fda4519b491473e491ae3e136d01c16662beccac0583b4e8806.jpg",
        "image_caption": [
            "Figure 12: Debate accuracy curves across various datasets using the Llama2-70B model. "
        ],
        "image_footnote": [],
        "page_idx": 17
    },
    {
        "type": "image",
        "img_path": "images/3cef86ab59be6b3596abe25ee0b14c1a278f3ab7905a08666cb62fdfac82c192.jpg",
        "image_caption": [
            "Figure 13: Debate accuracy curves across various datasets using the GPT-3.5-Turbo model. "
        ],
        "image_footnote": [],
        "page_idx": 17
    },
    {
        "type": "table",
        "img_path": "images/bf417b7724adad239f527f84d2847717f05be784de156259ea8b65ad131f48d3.jpg",
        "table_caption": [
            "Table 7: Statistical test results (p-values) of different LLMs on different datasets. One-way ANOVA is performed where ensemble sizes are (1, 10, 20, 30, 40). "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>LLMs</td><td>GSM8K</td><td>MATH</td><td>Chess</td><td>MMLU</td><td>HumanEval</td></tr><tr><td>Llama2-13B</td><td>1.56e-44</td><td>9.11e-32</td><td>1.97e-18</td><td>7.86e-38</td><td>5.46e-8</td></tr><tr><td>Llama2-70B</td><td>1.93e-43</td><td>2.16e-29</td><td>1.09e-9</td><td>1.06e-26</td><td>5.24e-7</td></tr><tr><td>GPT-3.5-Turbo</td><td>6.03e-30</td><td>9.66e-39</td><td>4.62e-24</td><td>2.46e-39</td><td>7.47e-13</td></tr></table>",
        "page_idx": 17
    },
    {
        "type": "image",
        "img_path": "images/40599167e3cff4c5a1b983088b47f9e4330bf9c864decc3b15bca2ecf61af4c6.jpg",
        "image_caption": [
            "Figure 14: Token usage vs. accuracy. "
        ],
        "image_footnote": [],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "B.3 Token Usage vs. Accuracy ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Figure 14 shows the tradeoff between token budget and accuracy, where the x-axis represents the token budget and the y-axis represents the accuracy. ",
        "page_idx": 17
    }
]